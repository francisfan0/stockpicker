{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98ef66a-fee5-4e6c-8dab-0a71da26b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14848\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        input_layer = 768\n",
    "        hidden_layer = 50\n",
    "        output_layer = 2\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        h_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = BertClassifier()\n",
    "model.load_state_dict(torch.load('stock_sentiment_model.pt'))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "                      \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737d442d-bf1a-426f-8610-31ce6fa272a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aal\n",
      "aapl\n",
      "adbe\n",
      "adp\n",
      "adsk\n",
      "akam\n",
      "alxn\n",
      "amat\n",
      "amgn\n",
      "amzn\n",
      "atvi\n",
      "avgo\n",
      "bbby\n",
      "bidu\n",
      "bmrn\n",
      "ca\n",
      "celg\n",
      "cern\n",
      "chkp\n",
      "chtr\n",
      "cmcsa\n",
      "cost\n",
      "csco\n",
      "csx\n",
      "ctrp\n",
      "ctsh\n",
      "disca\n",
      "disck\n",
      "dish\n",
      "dltr\n",
      "ea\n",
      "ebay\n",
      "endp\n",
      "esrx\n",
      "expe\n",
      "fast\n",
      "fb\n",
      "fisv\n",
      "foxa\n",
      "fox\n",
      "gild\n",
      "googl\n",
      "goog\n",
      "hsic\n",
      "ilmn\n",
      "inct\n",
      "incy\n",
      "intu\n",
      "isrg\n",
      "jd\n",
      "khc\n",
      "lbtya\n",
      "lbtyk\n",
      "lltc\n",
      "lmca\n",
      "lmck\n",
      "lrcx\n",
      "lrcx\n",
      "lvnta\n",
      "mar\n",
      "mat\n",
      "mdlz\n",
      "mnst\n",
      "msft\n",
      "mu\n",
      "mxim\n",
      "myl\n",
      "nclh\n",
      "nflx\n",
      "ntap\n",
      "ntes\n",
      "nvda\n",
      "nxpi\n",
      "orly\n",
      "payx\n",
      "pcar\n",
      "pcln\n",
      "pypl\n",
      "qcom\n",
      "qvca\n",
      "regn\n",
      "rost\n",
      "sbac\n",
      "sbux\n",
      "sndk\n",
      "srcl\n",
      "stx\n",
      "swks\n",
      "symc\n",
      "tmus\n",
      "trip\n",
      "tsco\n",
      "tsla\n",
      "txn\n",
      "ulta\n",
      "viab\n",
      "vod\n",
      "vrsk\n",
      "vrtx\n",
      "wba\n",
      "wdc\n",
      "wfm\n",
      "xlnx\n",
      "yhoo\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir('data/data1/'):\n",
    "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):  \n",
    "        parts = filename.split('_')\n",
    "\n",
    "        new_name = parts[2]\n",
    "        print(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a7d84c4-9d11-47e3-8cda-d5cd2ce8f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aal - completed\n",
      "aapl - completed\n",
      "adbe - completed\n",
      "adp - completed\n",
      "adsk - completed\n",
      "akam - completed\n",
      "alxn - completed\n",
      "amat - completed\n",
      "amgn - completed\n",
      "amzn - completed\n",
      "atvi - completed\n",
      "avgo - completed\n",
      "bbby - completed\n",
      "bidu - completed\n",
      "bmrn - completed\n",
      "ca - completed\n",
      "celg - completed\n",
      "cern - completed\n",
      "chkp - completed\n",
      "chtr - completed\n",
      "cmcsa - completed\n",
      "cost - completed\n",
      "csco - completed\n",
      "csx - completed\n",
      "ctrp - completed\n",
      "ctsh - completed\n",
      "disca - completed\n",
      "disck - completed\n",
      "dish - completed\n",
      "dltr - completed\n",
      "ea - completed\n",
      "ebay - completed\n",
      "endp - completed\n",
      "esrx - completed\n",
      "expe - completed\n",
      "fast - completed\n",
      "fb - completed\n",
      "fisv - completed\n",
      "foxa - completed\n",
      "fox - completed\n",
      "gild - completed\n",
      "googl - completed\n",
      "goog - completed\n",
      "hsic - completed\n",
      "ilmn - completed\n",
      "inct - completed\n",
      "incy - completed\n",
      "intu - completed\n",
      "isrg - completed\n",
      "jd - completed\n",
      "khc - completed\n",
      "lbtya - completed\n",
      "lbtyk - completed\n",
      "lltc - completed\n",
      "lmca - completed\n",
      "lmck - completed\n",
      "lrcx - completed\n",
      "lrcx - completed\n",
      "lvnta - completed\n",
      "mar - completed\n",
      "mat - completed\n",
      "mdlz - completed\n",
      "mnst - completed\n",
      "msft - completed\n",
      "mu - completed\n",
      "mxim - completed\n",
      "myl - completed\n",
      "nclh - completed\n",
      "nflx - completed\n",
      "ntap - completed\n",
      "ntes - completed\n",
      "nvda - completed\n",
      "nxpi - completed\n",
      "orly - completed\n",
      "payx - completed\n",
      "pcar - completed\n",
      "pcln - completed\n",
      "pypl - completed\n",
      "qcom - completed\n",
      "qvca - completed\n",
      "regn - completed\n",
      "rost - completed\n",
      "sbac - completed\n",
      "sbux - completed\n",
      "sndk - completed\n",
      "srcl - completed\n",
      "stx - completed\n",
      "swks - completed\n",
      "symc - completed\n",
      "tmus - completed\n",
      "trip - completed\n",
      "tsco - completed\n",
      "tsla - completed\n",
      "txn - completed\n",
      "ulta - completed\n",
      "viab - completed\n",
      "vod - completed\n",
      "vrsk - completed\n",
      "vrtx - completed\n",
      "wba - completed\n",
      "wdc - completed\n",
      "wfm - completed\n",
      "xlnx - completed\n",
      "yhoo - completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor\n",
    "\n",
    "def preprocess_tweet(row):\n",
    "    text = row['Text']\n",
    "    text = preprocessor.clean(text)\n",
    "    return text\n",
    "\n",
    "# Get the list of stock data to convert\n",
    "directory = \"data/data1/\"  # Replace with the actual directory path\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in data:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):  # Check if the file is an Excel file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        stock = pd.read_excel(file_path, sheet_name='Stream')\n",
    "\n",
    "        # Assign the ticker name as a column\n",
    "        stock['Ticker'] = filename.split('_')[0]\n",
    "\n",
    "        # Convert string date times to datetime\n",
    "        stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "        stock['Hour'] = stock['Hour'].apply(lambda t: pd.Timedelta(hours=int(t[:2]), minutes=int(t[3:])))\n",
    "        stock['Datetime'] = stock['Date'] + stock['Hour']\n",
    "\n",
    "        # Rename column that holds the tweets content\n",
    "        stock.rename(columns={'Tweet content': 'Text'}, inplace=True)\n",
    "\n",
    "        # Preprocess the tweet content\n",
    "        stock['Text_Cleaned'] = stock.apply(preprocess_tweet, axis=1)\n",
    "\n",
    "        # Remove excess columns\n",
    "        stock = stock[['Tweet Id', 'Ticker', 'Datetime', 'Text', 'Text_Cleaned', 'Favs', 'RTs', 'Followers', 'Following', 'Is a RT']]\n",
    "\n",
    "        # Fill NAs in Favs, RTs, Followers, and Following with 0\n",
    "        stock = stock.fillna(0)\n",
    "\n",
    "        # Encode processed tweets for the Bert NLP model\n",
    "        stock_inputs, stock_masks = preprocessing_for_bert(stock['Text_Cleaned'].values)\n",
    "\n",
    "        # Put stock data in PyTorch dataloader for processing\n",
    "        stock_data = TensorDataset(stock_inputs, stock_masks)\n",
    "        stock_sampler = RandomSampler(stock_data)\n",
    "        stock_dataloader = DataLoader(stock_data, sampler=stock_sampler, batch_size=16)\n",
    "\n",
    "        # Assign model to evaluate\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        # For each batch\n",
    "        for batch in stock_dataloader:\n",
    "            # Get encoded inputs and masks\n",
    "            batch_inputs, batch_masks = batch\n",
    "\n",
    "            # Send variables to device (GPU if available)\n",
    "            batch_inputs = batch_inputs.to(torch.device('cuda'))\n",
    "            batch_masks = batch_masks.to(torch.device('cuda'))\n",
    "\n",
    "            # Predict classes with Bert for given inputs\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "            # Convert predictions to 0s and 1s\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            predictions.append(preds)\n",
    "\n",
    "        # Combine all batch predictions\n",
    "        predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "        # Add predictions to stock dataframe\n",
    "        stock['Sentiment'] = predictions\n",
    "        \n",
    "        parts = filename.split('_')\n",
    "\n",
    "        # Get the desired part, which is the second element (index 1)\n",
    "        new_name = parts[2]\n",
    "\n",
    "        # Save predictions as a new CSV\n",
    "        stock.to_csv('data/raw/' + new_name + '_stock_data_sentiment.csv', index=False)\n",
    "\n",
    "        # Show stock names as they are completed\n",
    "        print(new_name.split('_')[0], '- completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd76a33-9fc8-4718-85b5-c8f36d9a598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime as dt\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bdb57a-f7e8-4156-9f62-f70f4c1ac97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw/aal\n",
      "data/raw/aapl\n",
      "data/raw/adbe\n",
      "data/raw/adp\n",
      "data/raw/adsk\n",
      "data/raw/akam\n",
      "data/raw/alxn\n",
      "data/raw/amat\n",
      "data/raw/amgn\n",
      "data/raw/amzn\n",
      "data/raw/atvi\n",
      "data/raw/avgo\n",
      "data/raw/bbby\n",
      "data/raw/bidu\n",
      "data/raw/bmrn\n",
      "data/raw/ca\n",
      "data/raw/celg\n",
      "data/raw/cern\n",
      "data/raw/chkp\n",
      "data/raw/chtr\n",
      "data/raw/cmcsa\n",
      "data/raw/cost\n",
      "data/raw/csco\n",
      "data/raw/csx\n",
      "data/raw/ctrp\n",
      "data/raw/ctsh\n",
      "data/raw/disca\n",
      "data/raw/disck\n",
      "data/raw/dish\n",
      "data/raw/dltr\n",
      "data/raw/ea\n",
      "data/raw/ebay\n",
      "data/raw/endp\n",
      "data/raw/esrx\n",
      "data/raw/expe\n",
      "data/raw/fast\n",
      "data/raw/fb\n",
      "data/raw/fisv\n",
      "data/raw/foxa\n",
      "data/raw/fox\n",
      "data/raw/gild\n",
      "data/raw/googl\n",
      "data/raw/goog\n",
      "data/raw/hsic\n",
      "data/raw/ilmn\n",
      "data/raw/inct\n",
      "data/raw/incy\n",
      "data/raw/intu\n",
      "data/raw/isrg\n",
      "data/raw/jd\n",
      "data/raw/khc\n",
      "data/raw/lbtya\n",
      "data/raw/lbtyk\n",
      "data/raw/lltc\n",
      "data/raw/lmca\n",
      "data/raw/lmck\n",
      "data/raw/lrcx\n",
      "data/raw/lvnta\n",
      "data/raw/mar\n",
      "data/raw/mat\n",
      "data/raw/mdlz\n",
      "data/raw/mnst\n",
      "data/raw/msft\n",
      "data/raw/mu\n",
      "data/raw/mxim\n",
      "data/raw/myl\n",
      "data/raw/nclh\n",
      "data/raw/nflx\n",
      "data/raw/ntap\n",
      "data/raw/ntes\n",
      "data/raw/nvda\n",
      "data/raw/nxpi\n",
      "data/raw/orly\n",
      "data/raw/payx\n",
      "data/raw/pcar\n",
      "data/raw/pcln\n",
      "data/raw/pypl\n",
      "data/raw/qcom\n",
      "data/raw/qvca\n",
      "data/raw/regn\n",
      "data/raw/rost\n",
      "data/raw/sbac\n",
      "data/raw/sbux\n",
      "data/raw/sndk\n",
      "data/raw/srcl\n",
      "data/raw/stx\n",
      "data/raw/swks\n",
      "data/raw/symc\n",
      "data/raw/tmus\n",
      "data/raw/trip\n",
      "data/raw/tsco\n",
      "data/raw/tsla\n",
      "data/raw/txn\n",
      "data/raw/ulta\n",
      "data/raw/viab\n",
      "data/raw/vod\n",
      "data/raw/vrsk\n",
      "data/raw/vrtx\n",
      "data/raw/wba\n",
      "data/raw/wdc\n",
      "data/raw/wfm\n",
      "data/raw/xlnx\n",
      "data/raw/yhoo\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('data/raw/')\n",
    "\n",
    "stocks = pd.DataFrame()\n",
    "\n",
    "directory = \"data/raw/\"  \n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\") or filename.endswith(\".csv\"):  \n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        parts = filename.split('_')\n",
    "\n",
    "        new_name = parts[0]\n",
    "        print('data/raw/' + new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3011e349-107a-4025-8065-1919982ee368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "aal - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "aapl - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "adbe - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "adp - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "adsk - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "akam - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- ALXN: No timezone found, symbol may be delisted\n",
      "alxn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "amat - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "amgn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "amzn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "atvi - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "avgo - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "bbby - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "bidu - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "bmrn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ca - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- CELG: No timezone found, symbol may be delisted\n",
      "celg - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- CERN: No timezone found, symbol may be delisted\n",
      "cern - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "chkp - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "chtr - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "cmcsa - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "cost - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "csco - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "csx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- CTRP: No timezone found, symbol may be delisted\n",
      "ctrp - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ctsh - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- DISCA: No timezone found, symbol may be delisted\n",
      "disca - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- DISCK: No timezone found, symbol may be delisted\n",
      "disck - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "dish - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "dltr - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ea - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ebay - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- ENDP: No timezone found, symbol may be delisted\n",
      "endp - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "esrx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "expe - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "fast - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- FB: No timezone found, symbol may be delisted\n",
      "fb - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "fisv - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- FOXA: Data doesn't exist for startDate = 1459224000, endDate = 1466222400\n",
      "foxa - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- FOX: Data doesn't exist for startDate = 1459137600, endDate = 1466136000\n",
      "fox - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "gild - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "googl - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "goog - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "hsic - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ilmn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "inct - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "incy - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "intu - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "isrg - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "jd - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "khc - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "lbtya - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "lbtyk - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- LLTC: No data found for this date range, symbol may be delisted\n",
      "lltc - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- LMCA: No data found for this date range, symbol may be delisted\n",
      "lmca - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- LMCK: No timezone found, symbol may be delisted\n",
      "lmck - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "lrcx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- LVNTA: No data found for this date range, symbol may be delisted\n",
      "lvnta - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "mar - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "mat - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "mdlz - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "mnst - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "msft - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "mu - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- MXIM: No timezone found, symbol may be delisted\n",
      "mxim - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- MYL: No timezone found, symbol may be delisted\n",
      "myl - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nclh - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nflx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ntap - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ntes - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nvda - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "nxpi - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "orly - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "payx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "pcar - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- PCLN: No data found for this date range, symbol may be delisted\n",
      "pcln - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "pypl - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "qcom - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- QVCA: No data found for this date range, symbol may be delisted\n",
      "qvca - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "regn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "rost - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "sbac - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "sbux - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- SNDK: No data found for this date range, symbol may be delisted\n",
      "sndk - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "srcl - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "stx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "swks - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- SYMC: No timezone found, symbol may be delisted\n",
      "symc - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "tmus - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "trip - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "tsco - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "tsla - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "txn - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ulta - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- VIAB: No timezone found, symbol may be delisted\n",
      "viab - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "vod - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "vrsk - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "vrtx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "wba - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "wdc - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- WFM: No data found for this date range, symbol may be delisted\n",
      "wfm - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- XLNX: No timezone found, symbol may be delisted\n",
      "xlnx - Completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- YHOO: No data found for this date range, symbol may be delisted\n",
      "yhoo - Completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "files = os.listdir('data/raw/')\n",
    "\n",
    "stocks = pd.DataFrame()\n",
    "\n",
    "for filename in files:\n",
    "    if filename.endswith(\".csv\"):\n",
    "        try:\n",
    "            data = pd.read_csv('data/raw/' + filename)\n",
    "        \n",
    "            data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "\n",
    "            data.loc[data['Sentiment']==0, 'Sentiment'] = -1\n",
    "        \n",
    "            data['Tweets'] = 1\n",
    "            data['Weight'] = 1\n",
    "\n",
    "            data['Followers_Mean'] = data['Followers'].rolling(10000, min_periods=1).mean()\n",
    "            data['Followers_Std'] = data['Followers'].rolling(10000, min_periods=1).std()\n",
    "            data['Followers_Std'] = data['Followers_Std'].fillna(data['Followers_Std'].values[1])\n",
    "\n",
    "            data.loc[ (data['Followers']>=data['Followers_Mean']) & (data['Followers'] < (data['Followers_Mean']+data['Followers_Std'])), 'Weight'] += 1\n",
    "            data.loc[ (data['Followers']>=(data['Followers_Mean']+data['Followers_Std'])) & (data['Followers'] < (data['Followers_Mean']+data['Followers_Std']*2)), 'Weight'] += 2\n",
    "            data.loc[data['Followers']>=(data['Followers_Mean']+data['Followers_Std']*2), 'Weight'] += 3\n",
    "\n",
    "            data['RTs_Mean'] = data['RTs'].rolling(10000, min_periods=1).mean()\n",
    "            data['RTs_Std'] = data['RTs'].rolling(10000, min_periods=1).std()\n",
    "            data['RTs_Std'] = data['RTs_Std'].fillna(data['RTs_Std'].values[1])\n",
    "    \n",
    "            data.loc[ (data['RTs']>=data['RTs_Mean']) & (data['RTs'] < (data['RTs_Mean']+data['RTs_Std'])), 'Weight'] += 1\n",
    "            data.loc[ (data['RTs']>=(data['RTs_Mean']+data['RTs_Std'])) & (data['RTs'] < (data['RTs_Mean']+data['RTs_Std']*2)), 'Weight'] += 2\n",
    "            data.loc[data['RTs']>=(data['RTs_Mean']+data['RTs_Std']*2), 'Weight'] += 3\n",
    "\n",
    "            data['Sentiment_Weighted'] = data['Sentiment']*data['Weight']\n",
    "        \n",
    "            data = data.groupby([data.Datetime.dt.month, data.Datetime.dt.day]).sum(numeric_only=True)\n",
    "                    \n",
    "            data['Ticker'] = filename.split('_')[0]\n",
    "        \n",
    "            data['Date'] = pd.to_datetime([ str(x)+'/'+str(y)+'/2016' for (x,y) in data.index.values ])+datetime.timedelta(days=1)\n",
    "            \n",
    "            data = data.reset_index(drop=True)\n",
    "    \n",
    "            data['Sentiment_Weighted'] /= data['Tweets']\n",
    "        \n",
    "            data['Sentiment_MA'] = data['Sentiment_Weighted'].rolling(3, min_periods=1).mean()\n",
    "            data['Tweets_MA'] = data['Tweets'].rolling(3, min_periods=1).mean()\n",
    "\n",
    "            start_date = data['Date'].min()\n",
    "            end_date = data['Date'].max()+datetime.timedelta(days=2)\n",
    "\n",
    "            prices = yf.download(tickers=filename.split('_')[0], start=start_date, end=end_date).reset_index()\n",
    "        \n",
    "            prices['Percent_Change'] = (prices['Adj Close'].pct_change()*100).shift(-1)\n",
    "        \n",
    "            prices['Percent_Change_Bin'] = pd.cut(prices['Percent_Change'], [-100, 0, 2, 100], labels=[0, 1, 2])\n",
    "\n",
    "            data = data.merge(prices, on='Date', how='left')\n",
    "\n",
    "            data = data[['Ticker', 'Date', 'Sentiment_Weighted', 'Sentiment_MA', 'Tweets', 'Tweets_MA', 'Adj Close', 'Percent_Change', 'Percent_Change_Bin']]\n",
    "        \n",
    "            data = data.dropna().reset_index(drop=True)\n",
    "        \n",
    "            data.to_csv('data/processed/' + filename.split('_')[0] + '_stock_data_inputs.csv', index=False)\n",
    "        \n",
    "            stocks = pd.concat([stocks, data])\n",
    "        \n",
    "            print(filename.split('_')[0], '- Completed')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(filename.split('_')[0], '-', e)\n",
    "    \n",
    "\n",
    "stocks.to_csv('data/processed/combined_stock_inputs1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f0c60-e2bc-45fb-a8c4-9fdd1d038656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
